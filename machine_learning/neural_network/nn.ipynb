{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2f464fc6b098c045d3fa2a81389409b13ae3524050d1c22ae6b1fc19c12f7a78"
   }
  },
  "interpreter": {
   "hash": "2f464fc6b098c045d3fa2a81389409b13ae3524050d1c22ae6b1fc19c12f7a78"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用numpy自带的tanh函数和softmax函数\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def soft_max(x):\n",
    "    exp = np.exp(x - x.max())\n",
    "    return exp / exp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [28*28, 10]\n",
    "activations = [tanh, soft_max]\n",
    "# 第一层和第二层的分布，w的分布参考浙大胡老师的课程\n",
    "distribution = [\n",
    "    {'b': [0, 0]},\n",
    "    {'b': [0, 0], 'w': [-math.sqrt(6/(dimensions[0]+dimensions[1])), math.sqrt(6/(dimensions[0]+dimensions[1]))]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters_b(layer):\n",
    "    dist = distribution[layer]['b']\n",
    "    # 给出dist[0]到dist[1]之间的随机数\n",
    "    return np.random.rand(dimensions[layer]) * (dist[1] -dist[0]) + dist[0]\n",
    "\n",
    "def init_parameters_w(layer):\n",
    "    dist = distribution[layer]['w']\n",
    "    return np.random.rand(dimensions[layer-1], dimensions[layer]) * (dist[1] -dist[0]) + dist[0]\n",
    "\n",
    "def init_parameters():\n",
    "    parameter = []\n",
    "    for i in range(len(distribution)):\n",
    "        layer_parameter = {}\n",
    "        for j in distribution[i].keys():\n",
    "            if j == 'b':\n",
    "                layer_parameter['b'] = init_parameters_b(i)\n",
    "                continue\n",
    "            if j == 'w':\n",
    "                layer_parameter['w'] = init_parameters_w(i)\n",
    "                continue\n",
    "        parameter.append(layer_parameter)\n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = init_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img, parameters):\n",
    "    layer0_in = img+parameters[0]['b']\n",
    "    layer0_out = activations[0](layer0_in)\n",
    "\n",
    "    layer1_in = np.dot(layer0_out, parameters[1]['w']) + parameters[1]['b']\n",
    "    layer1_out = activations[1](layer1_in)\n",
    "    return layer1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "predict(np.random.rand(28*28), parameters).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('./../mnist')\n",
    "train_img_path = dataset_path/'train-images.idx3-ubyte'\n",
    "train_lab_path = dataset_path/'train-labels.idx1-ubyte'\n",
    "test_img_path = dataset_path/'t10k-images.idx3-ubyte'\n",
    "test_lab_path = dataset_path/'t10k-labels.idx1-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以二进制文件读取\n",
    "train_f = open(train_img_path, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_num = 50000\n",
    "valid_num = 10000\n",
    "test_num = 10000\n",
    "\n",
    "with open(train_img_path, 'rb') as f:\n",
    "    struct.unpack('>4i', f.read(16))\n",
    "    temp_img = np.fromfile(f, dtype=np.uint8).reshape(-1, 28*28)\n",
    "    train_img = temp_img[:trian_num]\n",
    "    valid_img = temp_img[trian_num:]\n",
    "\n",
    "with open(test_img_path, 'rb') as f:\n",
    "    struct.unpack('>4i', f.read(16))\n",
    "    test_img = np.fromfile(f, dtype=np.uint8).reshape(-1, 28*28)\n",
    "\n",
    "\n",
    "with open(train_lab_path, 'rb') as f:\n",
    "    struct.unpack('>2i', f.read(8))\n",
    "    temp_lab = np.fromfile(f, dtype=np.uint8)\n",
    "    train_lab = temp_lab[:trian_num]\n",
    "    valid_lab = temp_lab[trian_num:]\n",
    "\n",
    "with open(test_lab_path, 'rb') as f:\n",
    "    struct.unpack('>2i', f.read(8))\n",
    "    test_lab = np.fromfile(f, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train(index):\n",
    "    print('label : {}'.format(train_lab[index]), end='\\n ')\n",
    "    plt.imshow(train_img[index].reshape(28, 28), cmap='gray')\n",
    "\n",
    "def show_valid(index):\n",
    "    print('label : {}'.format(valid_lab[index]), end='\\n ')\n",
    "    plt.imshow(valid_img[index].reshape(28, 28), cmap='gray')\n",
    "\n",
    "def show_test(index):\n",
    "    print('label : {}'.format(test_lab[index]), end='\\n ')\n",
    "    plt.imshow(test_img[index].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_soft_max(data):\n",
    "    sm=soft_max(data)\n",
    "    return np.diag(sm) - np.outer(sm, sm)\n",
    "\n",
    "# def d_tanh(data):\n",
    "#     return np.diag(1/(np.cosh(data))**2)\n",
    "\n",
    "def d_tanh(data):\n",
    "    return 1/(np.cosh(data))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "differential = {soft_max: d_soft_max, tanh: d_tanh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.03103085, -0.00279373, -0.00759413, -0.02064299],\n",
       "       [-0.00279373,  0.07955019, -0.02064299, -0.05611347],\n",
       "       [-0.00759413, -0.02064299,  0.18076935, -0.15253222],\n",
       "       [-0.02064299, -0.05611347, -0.15253222,  0.22928869]])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "d_tanh([1, 2, 3, 4])\n",
    "d_soft_max(np.array([1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-4.87366553e-09  1.70216383e-09  1.35511823e-09  7.06160429e-10]\n[ 1.44830090e-09 -4.89978003e-09  3.38688366e-10  1.16990049e-09]\n[ 5.64259472e-10  1.09531345e-09 -3.58784968e-09  1.37316528e-09]\n[ 1.37166211e-09  1.70217766e-09  1.30796536e-09 -4.65936087e-09]\n"
     ]
    }
   ],
   "source": [
    "h = 0.0000001\n",
    "func = soft_max\n",
    "input_len = 4\n",
    "\n",
    "for i in range(input_len):\n",
    "    test_input = np.random.rand(input_len)\n",
    "    derivative = differential[func](test_input)\n",
    "    value1 = func(test_input)\n",
    "    test_input[i]+= h\n",
    "    value2 = func(test_input)\n",
    "    # print((value2 - value1)/h)\n",
    "    # print(derivative[i])\n",
    "    print(derivative[i] - (value2 - value1) / h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3.50129558e-07 8.15759640e-01 8.15759640e-01 8.15759640e-01]\n[8.50046510e-01 3.29240547e-07 8.50046510e-01 8.50046510e-01]\n[9.20867646e-01 9.20867646e-01 2.59112743e-07 9.20867646e-01]\n[8.50246227e-01 8.50246227e-01 8.50246227e-01 3.29010209e-07]\n"
     ]
    }
   ],
   "source": [
    "h = 0.000001\n",
    "func = tanh\n",
    "input_len = 4\n",
    "\n",
    "for i in range(input_len):\n",
    "    test_input = np.random.rand(input_len)\n",
    "    derivative = differential[func](test_input)\n",
    "    value1 = func(test_input)\n",
    "    test_input[i]+= h\n",
    "    value2 = func(test_input)\n",
    "    # print((value2 - value1)/h)\n",
    "    # print(derivative[i])\n",
    "    print(derivative[i] - (value2 - value1) / h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单位阵\n",
    "onehot = np.identity(dimensions[-1])\n",
    "\n",
    "def sqr_loss(img, lab, parameters):\n",
    "    y_pred = predict(img, parameters)\n",
    "    y = onehot[lab]\n",
    "    diff = y - y_pred\n",
    "    return np.dot(diff, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0763839470062817"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "sqr_loss(train_img[0], train_lab[0], parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_parameters(img, lab, parameters):\n",
    "    layer0_in = img+parameters[0]['b']\n",
    "    layer0_out = activations[0](layer0_in)\n",
    "\n",
    "    layer1_in = np.dot(layer0_out, parameters[1]['w']) + parameters[1]['b']\n",
    "    layer1_out = activations[1](layer1_in)\n",
    "    \n",
    "    diff = onehot[lab] - layer1_out\n",
    "    act1 = np.dot(differential[activations[1]](layer1_in), diff)\n",
    "\n",
    "    grad_b1 = -2 * act1\n",
    "    grad_w1 = -2 * np.outer(layer0_out, act1)\n",
    "    grad_b0 = -2 * differential[activations[0]](layer0_in) * np.dot(parameters[1]['w'], act1)\n",
    "\n",
    "    return {'w1' : grad_w1, 'b1': grad_b1, 'b0': grad_b0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'w1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'b1': array([ 2.85451828e-03,  5.63163366e-03,  8.47206297e-06,  3.89114666e-02,\n",
       "        -1.42770366e-01, -1.01433101e-03,  4.04320359e-02,  6.24767044e-03,\n",
       "         3.17740885e-02,  1.79248118e-02]),\n",
       " 'b0': array([ 9.10810212e-003, -7.35517704e-003,  1.20520036e-002,\n",
       "         9.36193659e-003,  8.38669288e-003,  1.27927298e-003,\n",
       "        -1.68559260e-004,  1.47535661e-003,  5.44590440e-004,\n",
       "        -9.15104127e-003,  1.01025867e-002, -5.37917008e-003,\n",
       "         1.18705116e-003, -2.18911404e-004,  8.63069876e-003,\n",
       "         1.13569947e-002,  3.63299983e-004, -1.22092445e-002,\n",
       "         1.27313198e-002, -5.91883432e-003, -5.72286023e-003,\n",
       "        -9.45568986e-003,  1.51510854e-004,  1.03606468e-002,\n",
       "         5.20801403e-004, -2.77877387e-004,  6.64097780e-004,\n",
       "        -2.09572503e-003,  5.37788258e-003, -1.27822293e-002,\n",
       "        -8.32013454e-003, -8.58166304e-003, -7.75782452e-003,\n",
       "        -4.18916036e-003,  9.48613008e-003, -4.45475355e-003,\n",
       "         5.83525400e-003, -2.77309139e-003,  1.06867032e-002,\n",
       "        -7.17067965e-003,  1.24319687e-002,  7.00405197e-004,\n",
       "         2.67081044e-003,  4.22544824e-003, -1.07228487e-002,\n",
       "        -2.79276354e-003, -1.32967787e-002,  2.69579530e-003,\n",
       "        -6.67176453e-003, -9.91088613e-004,  1.42374520e-002,\n",
       "         3.93436787e-003,  7.05600737e-003,  6.83910672e-003,\n",
       "        -1.24722296e-002,  6.77161534e-003,  6.93687741e-003,\n",
       "         1.14924052e-002,  5.52164391e-003,  8.53609003e-003,\n",
       "         2.94920450e-004, -5.05175356e-003, -3.30324396e-003,\n",
       "         5.32957976e-004,  7.20866482e-004,  2.22280869e-003,\n",
       "         8.20390813e-004, -1.29945857e-002,  7.95808811e-003,\n",
       "        -4.37362387e-003,  2.09683934e-003,  7.89854909e-003,\n",
       "        -2.22381393e-003,  1.42988632e-003,  5.39062282e-003,\n",
       "         4.25948214e-003, -5.46137125e-003, -5.48507135e-003,\n",
       "         1.37298453e-002, -1.05114402e-002,  1.58913947e-002,\n",
       "         1.36785976e-002, -7.58432740e-003, -5.25410624e-003,\n",
       "        -1.53656485e-004,  1.44813621e-002,  9.96060181e-003,\n",
       "         8.71288212e-003, -1.38350543e-002,  6.41695064e-004,\n",
       "        -9.01266493e-003, -5.64828657e-003, -6.42881806e-003,\n",
       "        -4.50006382e-003, -2.97179465e-003,  4.45772731e-003,\n",
       "        -1.12283958e-002,  3.04117378e-003,  9.58640408e-003,\n",
       "        -1.40209716e-002,  1.79873381e-003, -1.66502524e-003,\n",
       "         5.98237789e-003,  8.82138597e-003, -2.34742681e-003,\n",
       "         3.67478413e-003, -1.14840237e-002, -4.53256351e-003,\n",
       "         2.92704813e-003, -1.16410834e-002, -6.10614265e-004,\n",
       "         7.64818449e-003,  7.46694665e-003,  1.21358815e-002,\n",
       "         1.67388126e-002,  1.12395416e-003,  5.96031141e-003,\n",
       "         1.10176623e-002, -1.52975029e-003, -2.23584903e-003,\n",
       "         1.57265536e-003, -1.33881741e-002,  7.98424446e-003,\n",
       "        -3.12488989e-003, -2.92694771e-003,  1.15720553e-002,\n",
       "        -9.50866131e-003, -1.14178709e-002, -5.25076735e-003,\n",
       "        -4.67258595e-003, -8.49960088e-004,  3.52053085e-004,\n",
       "        -7.95616364e-003, -1.88317197e-002, -7.38498928e-004,\n",
       "        -1.15928201e-002,  4.68482137e-003,  7.23748225e-003,\n",
       "         6.04951967e-003,  9.94374859e-003,  6.10758730e-003,\n",
       "        -6.07304262e-003,  4.19320789e-003, -3.32379328e-003,\n",
       "         1.83827749e-002,  4.84685980e-003, -4.56270410e-003,\n",
       "        -5.31316222e-003, -3.70096880e-003,  8.78638296e-003,\n",
       "        -7.23876663e-003,  4.69604649e-003,  1.22437572e-003,\n",
       "        -4.99859140e-003,  7.16902769e-003, -8.85939847e-003,\n",
       "        -3.97508687e-003, -1.19759387e-003,  4.58110382e-003,\n",
       "        -7.92014314e-003, -4.59396953e-061, -1.10529978e-203,\n",
       "         2.96311178e-036,  3.34607517e-003, -8.30078619e-003,\n",
       "         1.12536519e-003, -1.01001423e-003,  5.00823704e-006,\n",
       "        -1.46998192e-002, -1.46038937e-002,  1.07326100e-002,\n",
       "         6.40093308e-003,  2.81499052e-056, -8.70229339e-073,\n",
       "         4.23808108e-003,  6.87804830e-003, -1.35227757e-002,\n",
       "         3.36946855e-003,  2.42367798e-003,  6.93987684e-003,\n",
       "        -7.63384646e-003, -1.25130515e-002, -2.04641323e-003,\n",
       "         1.15078123e-002,  4.86373641e-003,  2.21751224e-003,\n",
       "        -2.17509828e-003, -8.82685410e-003,  3.29749976e-106,\n",
       "         4.71386920e-159,  2.23313237e-036,  6.32901407e-003,\n",
       "        -1.44611898e-003,  8.72109647e-003,  6.13496360e-003,\n",
       "        -1.24909715e-002, -5.08492410e-003,  4.22779935e-003,\n",
       "         1.49925586e-002,  9.51306532e-003, -1.28970546e-111,\n",
       "         1.10559861e-143,  1.47653593e-002,  3.69698622e-003,\n",
       "        -2.33975687e-003,  6.55845693e-003, -1.13327910e-002,\n",
       "         1.60482430e-003,  2.66454990e-003,  1.27313282e-002,\n",
       "        -1.53620201e-002,  5.71469533e-003, -4.96918539e-003,\n",
       "         2.38163130e-003,  3.19672811e-003,  5.45260107e-004,\n",
       "         3.65827178e-135, -1.05528779e-184,  2.05832450e-037,\n",
       "        -1.48319372e-002,  5.00670653e-003, -4.72375801e-003,\n",
       "         3.75474416e-003, -7.75874247e-004,  7.21927847e-004,\n",
       "        -7.43345822e-003,  5.09206981e-003, -2.70744799e-003,\n",
       "        -2.70284350e-193, -1.29955812e-143, -1.26040856e-002,\n",
       "         8.80853927e-003,  4.34150004e-004, -5.62715115e-004,\n",
       "         5.64574502e-003,  4.25889490e-004,  8.68325552e-003,\n",
       "         1.61217659e-003,  6.32962648e-003,  1.91991570e-003,\n",
       "        -1.56540305e-003, -4.58934395e-003,  1.53403212e-002,\n",
       "         7.53616484e-026, -5.58495152e-223, -2.81478308e-143,\n",
       "         1.35372112e-002, -7.62127568e-003, -4.42705996e-003,\n",
       "        -1.02981634e-002, -6.29616839e-003, -7.07917941e-003,\n",
       "        -1.00739923e-002, -1.43847683e-003,  2.37305003e-003,\n",
       "        -9.26955255e-003,  7.18646660e-195,  4.66302385e-145,\n",
       "         1.43589080e-002,  7.63246443e-003,  2.32927279e-004,\n",
       "        -5.20675070e-003,  1.25974462e-002,  5.96612920e-003,\n",
       "        -7.62452810e-003,  3.29735954e-003,  1.53610924e-002,\n",
       "         6.04692809e-003, -1.02630714e-002, -5.34892885e-003,\n",
       "        -1.03118401e-002, -7.24135850e-162,  8.43038385e-223,\n",
       "         7.53617808e-111,  8.36807863e-003,  4.76683802e-003,\n",
       "        -1.19524090e-002, -1.02100494e-002, -8.97672241e-003,\n",
       "        -9.80865189e-003,  1.76658550e-003, -1.41774288e-002,\n",
       "        -9.97340978e-003, -1.00742896e-042, -3.31404753e-215,\n",
       "         1.69485580e-144,  4.76988314e-003, -3.13910549e-003,\n",
       "        -8.06962872e-003, -1.18169415e-002, -4.53675158e-003,\n",
       "         9.81861485e-003, -9.76894219e-003, -4.95073944e-003,\n",
       "         3.26132971e-003,  1.26604053e-002, -1.00541311e-002,\n",
       "        -3.88015952e-003, -2.19292122e-003,  7.40503454e-174,\n",
       "         1.18861663e-222,  5.74977782e-052,  7.39527479e-003,\n",
       "        -1.36404019e-002,  8.34185202e-003,  1.08374325e-002,\n",
       "        -1.03946177e-002,  5.00938067e-003, -2.62670604e-003,\n",
       "        -6.55036005e-003,  9.86410144e-003,  1.07919751e-106,\n",
       "        -5.63679845e-224,  1.88076057e-144, -5.69595884e-003,\n",
       "         1.23048708e-002,  5.63523166e-003, -9.27725880e-004,\n",
       "        -1.25674165e-002,  2.94030544e-003, -6.89482415e-003,\n",
       "         1.42329593e-003,  1.25293128e-002,  3.83837766e-003,\n",
       "        -6.06937650e-003,  1.16518978e-002,  2.05786515e-022,\n",
       "        -9.76414677e-205,  6.84680051e-223,  1.30023655e-027,\n",
       "         5.01873301e-003, -2.30384399e-003,  4.42938681e-003,\n",
       "        -1.78394406e-003, -9.66453675e-003, -1.11484696e-002,\n",
       "        -1.63537870e-002,  1.93543965e-002,  6.46148325e-003,\n",
       "         5.99655157e-141,  5.36914663e-223,  2.48320218e-106,\n",
       "         2.81103410e-003,  6.14116426e-003,  7.58426431e-003,\n",
       "        -1.20031660e-002,  4.61404101e-003, -5.83596288e-003,\n",
       "        -4.63983017e-003,  3.24815188e-003, -1.26566652e-002,\n",
       "        -1.02248858e-002,  3.03499272e-003,  3.54525112e-003,\n",
       "         1.00015523e-143, -8.31914202e-223, -9.03255063e-190,\n",
       "        -2.15416611e-016, -6.24364491e-003, -6.92288492e-003,\n",
       "         9.28971976e-003,  8.99532474e-003,  1.10178911e-002,\n",
       "         6.26636369e-003, -1.13404713e-002,  4.83535856e-003,\n",
       "        -3.47849330e-004,  6.62298625e-141,  6.96787967e-223,\n",
       "        -2.92273653e-060,  1.13721886e-003,  1.21211835e-002,\n",
       "        -2.48514981e-003, -3.18115659e-003,  8.32683189e-003,\n",
       "        -3.90289741e-003,  8.09336077e-003,  7.82640172e-003,\n",
       "        -1.49738982e-002, -3.82090016e-014, -5.44664131e-077,\n",
       "         6.75193240e-157, -7.06912119e-218,  8.30686202e-223,\n",
       "        -5.17839215e-081, -7.52315043e-003, -7.63188820e-003,\n",
       "         2.17633124e-003, -1.25743184e-002,  9.48899432e-003,\n",
       "        -2.27251109e-003, -6.36388061e-003, -1.02510665e-002,\n",
       "        -1.46906073e-002,  4.15029464e-003,  4.05547478e-140,\n",
       "        -3.17597850e-223,  2.37338273e-076, -7.50002583e-003,\n",
       "         9.39809578e-003,  8.68462463e-003,  6.38992991e-043,\n",
       "        -1.01120739e-044,  3.06155420e-103,  1.31355285e-127,\n",
       "         1.75127723e-132, -1.63446243e-211,  3.22485863e-213,\n",
       "         1.38240697e-205, -1.18013361e-157, -2.60528230e-211,\n",
       "         3.91040872e-221,  3.34443515e-037, -1.33512085e-003,\n",
       "         4.11517479e-003,  3.64679955e-003,  8.33602020e-003,\n",
       "        -5.76382969e-003, -2.87789189e-003, -8.92282374e-003,\n",
       "         1.18457478e-002,  1.25836149e-002, -7.71687927e-003,\n",
       "         6.94953319e-133, -7.35740490e-222,  3.44663592e-208,\n",
       "         1.81440209e-182, -8.78622694e-183,  1.57876479e-182,\n",
       "         6.34034136e-222,  1.07402549e-222, -2.49267437e-219,\n",
       "         4.60458367e-211, -6.76194337e-175, -2.96640805e-126,\n",
       "        -4.13915125e-081,  2.44166852e-026, -2.32017276e-006,\n",
       "         1.52543470e-204,  2.31301348e-219,  7.71057685e-003,\n",
       "         1.33047516e-003, -1.21621174e-002,  5.37383836e-003,\n",
       "         5.59272131e-003, -1.71308571e-002,  2.65141884e-003,\n",
       "        -5.87438646e-003,  6.55824102e-003, -4.37797504e-003,\n",
       "        -1.92479460e-003, -9.89346166e-003,  1.52024134e-105,\n",
       "        -7.33713252e-156,  8.91687582e-156, -5.69359150e-156,\n",
       "        -5.84463715e-156,  3.17313777e-156, -3.35226748e-087,\n",
       "         7.24966163e-051,  1.18315230e-002,  6.31933056e-003,\n",
       "         9.72929166e-003, -1.54136865e-002, -2.99033891e-003,\n",
       "         2.91048223e-092, -9.54175668e-223, -2.87285452e-193,\n",
       "        -6.95023629e-003,  1.28436498e-002,  1.35430324e-002,\n",
       "         1.14760582e-002, -1.20447975e-002, -1.87270169e-003,\n",
       "        -9.05334720e-003, -5.79998272e-003,  5.18880906e-003,\n",
       "         5.75226985e-003,  3.34912671e-004, -1.01056549e-002,\n",
       "         9.79122331e-004,  9.84365222e-003,  7.99212657e-003,\n",
       "        -2.05669800e-003,  8.86271850e-003, -6.68278335e-003,\n",
       "         1.93585310e-003, -5.07408341e-003, -8.51000326e-003,\n",
       "         1.10552356e-002,  7.67744885e-003,  6.27261683e-003,\n",
       "        -1.11854791e-002, -1.00916867e-148,  4.68813895e-223,\n",
       "         2.98530181e-121,  2.37438902e-004,  1.13427310e-002,\n",
       "         4.08975152e-003,  1.06617557e-003, -8.45600865e-003,\n",
       "        -1.64432198e-003,  1.24336433e-002,  1.96917057e-003,\n",
       "        -4.03545952e-003,  3.71565165e-003,  3.91215507e-003,\n",
       "        -1.70499050e-002, -3.28918817e-003, -5.63000700e-003,\n",
       "         5.15390609e-003,  1.02601053e-002, -6.96605749e-003,\n",
       "        -4.35974778e-003, -1.77745703e-002,  5.77439336e-004,\n",
       "        -9.27032633e-003,  1.50400065e-003, -2.20274217e-003,\n",
       "        -7.62726502e-003,  5.63266548e-003, -8.81233213e-149,\n",
       "         1.04067658e-222, -3.86611136e-052,  3.96786749e-003,\n",
       "        -8.38816486e-003, -1.08879784e-002,  8.37533151e-003,\n",
       "        -2.43587881e-004,  6.11104465e-003,  2.18797169e-003,\n",
       "         7.35562166e-003,  3.43913937e-003, -2.05639044e-003,\n",
       "        -1.03784992e-002,  8.74201046e-003,  2.60751205e-003,\n",
       "        -3.79995799e-003,  2.76641591e-003, -2.08531503e-003,\n",
       "        -6.95011396e-003,  1.37393544e-002,  1.20829902e-002,\n",
       "        -2.73676855e-003,  1.65303008e-002,  7.60827623e-003,\n",
       "        -2.24005836e-003,  1.44610011e-003, -1.09587069e-002,\n",
       "         8.73425566e-149, -6.91458388e-224,  5.19573313e-052,\n",
       "         7.71629805e-004, -5.77114489e-003, -8.68964492e-003,\n",
       "        -2.49228237e-003,  6.40911095e-003,  1.34522844e-002,\n",
       "        -8.31284118e-004, -9.01155512e-003,  8.01431377e-003,\n",
       "        -1.01697629e-002,  1.21943767e-002, -7.81476532e-003,\n",
       "         4.82420806e-003,  1.12924867e-002, -7.67850452e-003,\n",
       "         7.02316212e-004,  5.91648495e-003,  6.22176243e-003,\n",
       "        -4.05379178e-003, -1.78316529e-003, -6.72662972e-003,\n",
       "         1.07538244e-002,  1.01129285e-002,  6.31375615e-003,\n",
       "        -9.49102534e-003, -2.15314926e-149, -7.37844696e-224,\n",
       "         5.29180007e-084, -6.82179456e-003,  2.19745509e-003,\n",
       "        -6.26389340e-004,  5.67126473e-003,  1.06044260e-003,\n",
       "        -6.61609391e-003, -7.32466763e-003, -5.34924104e-003,\n",
       "         2.63228902e-003, -1.04810528e-002,  1.46718210e-002,\n",
       "         1.19694940e-002,  8.06494385e-005, -2.32652746e-003,\n",
       "         1.11537158e-002,  3.77227103e-003, -9.76838044e-003,\n",
       "        -1.00501348e-002, -7.08842769e-003, -1.57086197e-002,\n",
       "         7.36538507e-003,  1.14416151e-002,  7.58765868e-003,\n",
       "        -9.79516983e-003, -8.73717498e-004, -2.67852628e-149,\n",
       "         9.02917530e-223, -9.33049671e-086, -7.22712981e-003,\n",
       "        -6.39629573e-003, -1.09295540e-002,  7.29755560e-003,\n",
       "         7.88626870e-003, -5.02507435e-003, -2.59238150e-003,\n",
       "        -7.44811517e-003, -4.60912132e-003, -9.01442954e-003,\n",
       "         2.63555306e-003, -4.53822170e-003, -1.32569192e-002,\n",
       "        -1.20795017e-004,  7.18967152e-004, -4.11769261e-003,\n",
       "        -1.28070073e-002,  6.17859168e-003, -1.42839490e-003,\n",
       "        -8.89038191e-003,  5.10938607e-003,  1.21914182e-002,\n",
       "         2.36357851e-003, -1.15298772e-002,  1.05980628e-004,\n",
       "         8.07851467e-149, -4.19588486e-223,  8.75516411e-137,\n",
       "        -6.76605837e-003, -6.48132242e-003, -4.48895035e-003,\n",
       "         1.79270952e-002,  1.02916231e-002,  1.49666726e-002,\n",
       "         9.58903660e-006,  1.03525646e-003, -3.81857593e-003,\n",
       "        -4.48784984e-003,  8.72229341e-003,  1.28674381e-003,\n",
       "         4.30236412e-003, -1.25256167e-003,  8.54897582e-003,\n",
       "        -8.33037876e-003,  1.27134756e-002,  1.00047891e-002,\n",
       "         6.90328961e-003, -1.92692948e-003, -2.51471899e-004,\n",
       "         1.23843311e-002,  5.46322744e-003,  9.18587580e-005,\n",
       "        -9.18388714e-003,  3.07539998e-149,  1.55298439e-223,\n",
       "        -5.35918828e-136, -1.01575218e-002, -1.45599293e-002,\n",
       "        -7.01749956e-003,  3.95422428e-003, -9.20464532e-003,\n",
       "        -3.35730282e-003, -1.16954601e-002, -9.02482270e-003,\n",
       "         4.89498068e-003, -6.47789087e-003,  1.84751030e-002,\n",
       "         1.08130607e-002,  2.89977332e-003, -2.11024128e-003,\n",
       "        -4.58400752e-003, -2.04818722e-003, -6.87990824e-003,\n",
       "         1.32012943e-002, -6.87944317e-003,  2.77330397e-003,\n",
       "        -6.38808907e-003, -1.20701244e-002, -7.73885620e-003,\n",
       "         4.13834299e-004,  2.13405412e-003, -7.96944638e-087,\n",
       "        -9.11286466e-223,  4.61161365e-135,  3.93266479e-003,\n",
       "         3.33609746e-003,  1.21773103e-003, -2.36823972e-003,\n",
       "         7.11466229e-003,  2.35338695e-003, -6.55626846e-003,\n",
       "        -1.35574859e-002,  6.18615435e-003,  1.86468775e-003,\n",
       "        -1.35468746e-002,  1.14976405e-002, -1.57363695e-002,\n",
       "        -2.70300414e-003, -1.07032400e-003, -1.21194457e-002,\n",
       "         3.36278212e-003,  6.81783222e-003, -4.93545295e-003,\n",
       "         6.24437915e-004,  3.79822077e-003, -3.62111194e-003,\n",
       "         3.82533183e-003,  6.25454437e-003,  8.29289462e-003,\n",
       "        -5.51115027e-004,  1.17354992e-002,  7.18575400e-003,\n",
       "        -1.07068506e-002, -4.74583214e-005, -1.05703638e-002,\n",
       "        -1.07966730e-002, -5.25559388e-003, -4.36044454e-003,\n",
       "        -1.61348709e-002, -2.96287425e-003, -1.08668008e-002,\n",
       "         8.17164754e-003,  6.83847528e-003, -9.30422617e-003,\n",
       "         1.00950951e-003,  7.77612038e-003, -1.28679894e-002,\n",
       "        -1.01851297e-002,  1.12548836e-002,  6.93745286e-003,\n",
       "         1.20649459e-002,  5.23514087e-003,  5.25478629e-003,\n",
       "        -2.84605161e-003, -9.43142404e-003, -7.72323047e-003,\n",
       "         1.23402873e-003,  1.50084579e-002,  6.02783253e-003,\n",
       "        -7.03382268e-003, -8.70208025e-003, -4.42255135e-003,\n",
       "         7.73439108e-003,  6.28905675e-004,  1.10034473e-002,\n",
       "         4.86207926e-003,  1.61068732e-003,  9.40597881e-003,\n",
       "        -3.52386216e-004,  7.14118785e-004,  1.44044891e-002,\n",
       "        -6.65027805e-003, -5.57175428e-004, -1.23245046e-002,\n",
       "        -1.41065961e-002, -9.82643586e-003,  1.28424903e-002,\n",
       "        -2.34617625e-003,  1.03071876e-002, -2.86708297e-004,\n",
       "        -1.73305550e-003,  1.17298479e-002, -4.80810326e-003,\n",
       "        -1.33399273e-002,  1.35676644e-003,  3.17484470e-003,\n",
       "        -6.59939716e-003,  1.58811056e-002,  9.63609691e-003,\n",
       "         1.04619149e-002, -1.93941850e-004,  1.03227233e-002,\n",
       "        -3.57668856e-003,  6.19954182e-004, -4.04797330e-003,\n",
       "        -1.12375892e-002])}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "grad_parameters(train_img[2], train_lab[2], init_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-6.542451023741624e-05\n",
      "5.6187045760380805e-05\n",
      "-2.0339701125818288e-05\n",
      "-1.1198830974029628e-05\n",
      "-4.1904865633199395e-05\n",
      "-1.3602850310785036e-05\n",
      "-1.2816827052154756e-05\n",
      "-6.7069330874818325e-06\n",
      "-2.4191521874723787e-05\n",
      "-5.106199042380355e-05\n"
     ]
    }
   ],
   "source": [
    "# b1\n",
    "h = 0.001\n",
    "for i in range(10):\n",
    "    img_i = np.random.randint(trian_num)\n",
    "    test_parameters = init_parameters()\n",
    "    derivative = grad_parameters(train_img[img_i], train_lab[img_i], test_parameters)['b1']\n",
    "    value1 = sqr_loss(train_img[img_i], train_lab[img_i], test_parameters)\n",
    "    test_parameters[1]['b'][i]+= h\n",
    "    value2 = sqr_loss(train_img[img_i], train_lab[img_i], test_parameters)\n",
    "    print(derivative[i] - (value2 - value1) / h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.067388544725663e-07"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# w1\n",
    "grad_list = []\n",
    "h = 0.00001\n",
    "for i in range(28*28):\n",
    "    for j in range(10):\n",
    "        img_i = np.random.randint(trian_num)\n",
    "        test_parameters = init_parameters()\n",
    "        derivative = grad_parameters(train_img[img_i], train_lab[img_i], test_parameters)['w1']\n",
    "        value1 = sqr_loss(train_img[img_i], train_lab[img_i], test_parameters)\n",
    "        test_parameters[1]['w'][i][j] += h\n",
    "        value2 = sqr_loss(train_img[img_i], train_lab[img_i], test_parameters)\n",
    "        grad_list.append(derivative[i][j] - (value2 - value1) / h)\n",
    "np.abs(grad_list).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.3017149954296092e-08"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# b0\n",
    "grad_list = []\n",
    "h = 0.00001\n",
    "for i in range(28*28):\n",
    "    img_i = np.random.randint(trian_num)\n",
    "    test_parameters = init_parameters()\n",
    "    derivative = grad_parameters(train_img[img_i], train_lab[img_i], test_parameters)['b0']\n",
    "    value1 = sqr_loss(train_img[img_i], train_lab[img_i], test_parameters)\n",
    "    test_parameters[0]['b'][i]+= h\n",
    "    value2 = sqr_loss(train_img[img_i], train_lab[img_i], test_parameters)\n",
    "    grad_list.append(derivative[i] - (value2 - value1) / h)\n",
    "np.abs(grad_list).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精确度相关\n",
    "\n",
    "def valid_loss(parameters):\n",
    "    loss_accu = 0\n",
    "    for img_i in range(valid_num):\n",
    "        loss_accu += sqr_loss(valid_img[img_i], valid_lab[img_i], parameters)\n",
    "    return loss_accu\n",
    "\n",
    "def valid_accuracy(parameters):\n",
    "    correct = [predict(valid_img[img_i], parameters).argmax() == valid_lab[img_i] for img_i in range(valid_num)]\n",
    "    print('validation accuracy: {}'.format((correct.count(True)/len(correct)*100)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "validation accuracy: 7.4399999999999995 %\n"
     ]
    }
   ],
   "source": [
    "valid_loss(parameters)\n",
    "valid_accuracy(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100张图片的梯度平均值\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "def train_batch(currect_batch, parameters):\n",
    "    grad_accu = grad_parameters(train_img[currect_batch*batch_size+0], train_lab[currect_batch*batch_size+0], test_parameters)\n",
    "    for img_i in range(1, batch_size):\n",
    "        grad_tmp = grad_parameters(train_img[currect_batch*batch_size+img_i], train_lab[currect_batch*batch_size+img_i], test_parameters)\n",
    "        for key in grad_accu.keys():\n",
    "            grad_accu[key] += grad_tmp[key]\n",
    "    for key in grad_accu.keys():\n",
    "        grad_accu[key] /= batch_size\n",
    "    return grad_accu\n",
    "\n",
    "import copy\n",
    "\n",
    "def combine_parameters(parameters, grad, learn_rate):\n",
    "    parameters_tmp = copy.deepcopy(parameters)\n",
    "    parameters_tmp[0]['b'] -= learn_rate * grad['b0']\n",
    "    parameters_tmp[1]['b'] -= learn_rate * grad['b1']\n",
    "    parameters_tmp[1]['w'] -= learn_rate * grad['w1']\n",
    "    return parameters_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'b': array([-1.99318648e-03, -2.91135261e-03,  1.97094586e-04, -1.89776964e-03,\n",
       "          3.89504426e-04, -1.65481944e-03, -2.12717188e-03,  1.01193842e-03,\n",
       "         -1.31585455e-03,  5.74568002e-04, -1.93757586e-03, -5.31654312e-04,\n",
       "          3.53767839e-03, -1.57614570e-04, -4.22137937e-03,  4.51447429e-03,\n",
       "          4.79220495e-04,  1.79265761e-03,  2.68251432e-03, -6.31164029e-04,\n",
       "         -8.20796943e-04, -2.47500806e-03,  3.62083541e-03, -3.42103842e-04,\n",
       "          3.20702518e-03,  1.52163568e-03, -2.70001228e-03, -3.83205024e-03,\n",
       "         -2.60459895e-03,  2.99791675e-03,  2.62556258e-03,  3.16773581e-03,\n",
       "          2.90623759e-03,  3.98558645e-03, -3.73430071e-03, -2.84215809e-03,\n",
       "          2.10717316e-03,  3.87085526e-03, -1.86337309e-03,  2.78907675e-03,\n",
       "          1.90770021e-03,  1.82975558e-03, -1.29266742e-03,  3.90865283e-03,\n",
       "          6.45706929e-04, -1.60497369e-04,  2.00986993e-03,  1.25713111e-03,\n",
       "          6.44888493e-04,  2.81004331e-04, -4.72026790e-04, -1.21188712e-03,\n",
       "         -1.38282917e-03, -9.98544040e-04,  3.50122269e-03,  9.56900632e-04,\n",
       "          8.46010624e-04,  3.11545692e-03, -1.64410999e-03,  4.98643280e-04,\n",
       "         -2.88172275e-03, -1.00604286e-03, -8.21068704e-06,  1.46271123e-03,\n",
       "         -2.39800747e-03, -3.85225254e-03,  2.64834898e-03,  2.19557253e-03,\n",
       "          1.84110397e-04, -2.93921951e-03, -2.16128533e-03,  9.54763954e-04,\n",
       "          9.48757358e-04, -2.58352712e-03,  3.63231926e-03,  9.06185766e-04,\n",
       "          2.93513615e-03,  2.58086154e-03, -1.79471929e-03,  8.05301277e-04,\n",
       "          2.80466389e-03,  5.96768277e-03,  1.80386201e-03, -1.16789376e-03,\n",
       "         -1.26827675e-04, -8.19199803e-04,  5.72903605e-03, -4.59929348e-03,\n",
       "         -1.19670426e-03,  3.42677947e-04,  2.69495350e-04,  1.87671347e-03,\n",
       "          1.22457190e-04,  7.96106868e-05,  3.14433373e-03,  9.78359867e-04,\n",
       "         -3.91522353e-03,  2.98759832e-03,  7.28982900e-05,  6.92097582e-04,\n",
       "          2.64579072e-03, -4.18043371e-03, -9.31126710e-04, -3.19025242e-03,\n",
       "         -2.92180717e-03,  2.11126102e-04, -3.40683197e-03, -5.49974103e-04,\n",
       "          1.68547711e-03, -2.52366803e-04,  1.89937442e-03,  1.82189640e-03,\n",
       "          4.97885180e-03,  1.96420451e-03,  8.56764345e-04, -9.46967134e-04,\n",
       "          1.18677835e-04,  1.18964193e-03, -2.05856635e-03, -2.34990226e-03,\n",
       "          2.39790321e-03,  3.27393881e-03, -2.33961273e-03, -2.89484961e-03,\n",
       "         -1.21822542e-03,  3.75142251e-04,  3.20725956e-05, -1.13784915e-05,\n",
       "         -2.13336777e-03, -1.87256332e-03,  1.74468852e-03, -5.49655055e-05,\n",
       "         -2.25448195e-03,  1.75102342e-03, -5.00476339e-03, -3.25251910e-03,\n",
       "          1.66753834e-03,  3.88918634e-03, -6.20507378e-04,  9.31905542e-04,\n",
       "          1.85326257e-04, -2.42027663e-03,  2.50272980e-04,  2.93044645e-03,\n",
       "          2.11994148e-03, -1.65823768e-03, -2.17160860e-03, -2.31403692e-03,\n",
       "         -2.90952962e-03, -1.06104391e-03,  3.06688791e-03, -9.52197290e-04,\n",
       "         -6.29662809e-04,  3.57929376e-04, -1.79300239e-03, -1.13666097e-03,\n",
       "         -1.08802617e-03, -1.19098060e-03,  3.59441084e-04, -8.57760949e-04,\n",
       "          7.96234015e-04,  1.33227622e-04,  4.91308173e-03, -3.39750876e-03,\n",
       "         -5.35775806e-03,  3.57222929e-03, -4.02024344e-03,  2.68973249e-03,\n",
       "          1.68331781e-03,  3.63783605e-03,  7.92746208e-04, -7.69764466e-04,\n",
       "          2.09095376e-03, -2.35702121e-03,  8.85060620e-04, -2.00188021e-03,\n",
       "         -2.64522750e-03, -1.18857497e-03, -4.70512151e-04,  2.79796713e-03,\n",
       "         -1.32306165e-03, -2.35300951e-04,  3.48637688e-04,  1.62902757e-03,\n",
       "          2.30780227e-03,  1.04458128e-03, -5.04301217e-04,  4.11674923e-04,\n",
       "          3.31610644e-03, -2.22520940e-03,  4.41087933e-03, -2.99411233e-04,\n",
       "          1.58713697e-03, -2.87090313e-03, -1.44029215e-04,  1.81720438e-03,\n",
       "         -1.77367224e-03,  4.77790771e-04, -1.71741306e-03,  4.23438757e-04,\n",
       "          3.29547406e-03,  3.37923748e-04,  2.91295810e-03,  2.92513260e-04,\n",
       "         -2.62922415e-03, -2.25845470e-03,  9.72080163e-04, -7.27940827e-04,\n",
       "          9.20453780e-04, -1.07579849e-03,  5.44794754e-04, -6.67240643e-04,\n",
       "         -2.08409953e-03,  1.01527181e-04, -1.58164612e-03,  1.17129451e-03,\n",
       "         -2.37353490e-03, -7.12019979e-04,  7.00190626e-04,  3.70695426e-03,\n",
       "          2.30024152e-03, -1.70021549e-03,  4.02646425e-03,  5.91338723e-04,\n",
       "          2.39623264e-03,  7.41371400e-04,  3.93684283e-04,  9.20516814e-04,\n",
       "         -4.08471474e-03, -6.57082543e-04, -2.01713136e-03, -1.63190201e-03,\n",
       "          1.93246816e-04,  2.24787554e-03,  2.51081587e-03, -1.43268015e-03,\n",
       "         -1.48629606e-03,  1.45530550e-04, -1.32927506e-04,  1.55724105e-03,\n",
       "         -2.25904600e-03,  1.05661375e-03,  2.44727113e-04, -9.35315924e-05,\n",
       "         -2.12429205e-03,  1.87310286e-03, -4.51319764e-04, -2.46408565e-03,\n",
       "         -4.62192351e-03, -4.36415892e-03,  8.98373027e-04, -1.16652061e-03,\n",
       "          5.52634725e-05, -1.41978173e-03, -4.22388886e-04, -2.24006449e-03,\n",
       "         -4.56074956e-04, -2.02917471e-03,  3.78574729e-03, -3.10367986e-03,\n",
       "         -8.32783932e-04, -6.65381896e-04,  9.80127103e-04, -1.02192710e-03,\n",
       "         -1.41620910e-03,  8.37880179e-04,  7.70527652e-05, -1.10409690e-03,\n",
       "         -4.08860827e-04, -2.23283623e-04, -2.09342903e-03,  2.01552845e-03,\n",
       "         -3.76582677e-03, -2.08429402e-04,  2.29909614e-03,  1.44538167e-04,\n",
       "         -3.82079256e-04, -1.95288414e-03, -2.12690516e-03,  3.01949102e-04,\n",
       "         -2.24663070e-03,  2.89243971e-03,  2.40971579e-03, -3.31857020e-03,\n",
       "         -1.22888551e-03,  3.14642898e-03, -4.53289309e-03,  1.50590178e-03,\n",
       "         -2.49253657e-03, -8.98186401e-04, -1.95126033e-04, -3.35107086e-03,\n",
       "         -6.40353013e-04,  1.48924519e-04,  4.32055099e-04,  1.97045850e-04,\n",
       "         -4.32511153e-04,  1.13029518e-03,  1.65188803e-04, -6.18005501e-04,\n",
       "         -2.45188316e-03, -4.25312822e-04,  2.94116182e-04, -3.28587371e-03,\n",
       "          4.02247765e-03,  3.50023535e-03, -1.49844896e-03,  8.95425708e-04,\n",
       "         -3.24376342e-03,  1.03164516e-03,  2.23896667e-04, -4.10778340e-04,\n",
       "         -8.30396328e-05, -2.53265958e-03,  3.31354945e-03,  3.98377728e-03,\n",
       "          3.04262981e-03, -1.52828645e-03,  2.65674464e-03, -1.03233005e-04,\n",
       "          2.69371553e-04,  1.07282840e-03,  8.95263732e-04,  6.35006400e-04,\n",
       "          1.24628156e-04, -3.70322631e-04,  4.31336440e-04, -5.19200879e-04,\n",
       "          3.72784216e-03, -6.60451705e-04, -1.00159997e-04, -2.09237202e-03,\n",
       "          2.94250158e-03, -3.78526929e-03, -1.09966601e-03,  3.00732952e-03,\n",
       "          2.17308912e-04,  9.39057967e-04,  7.95365741e-04,  1.85732238e-03,\n",
       "          3.22637151e-03, -1.64183426e-03, -2.81712323e-05, -2.72186524e-03,\n",
       "         -1.01354835e-03, -1.21523925e-03,  2.54124115e-03, -4.90793814e-04,\n",
       "         -3.72009342e-04,  9.38495560e-04,  6.34736794e-04,  6.44481920e-04,\n",
       "         -1.95309471e-03,  6.17470100e-04,  5.31827320e-05, -4.21865210e-04,\n",
       "         -1.88470184e-03, -1.27270775e-03,  2.02190856e-03,  1.23100763e-03,\n",
       "         -7.57832067e-04,  4.61487460e-04, -1.41038128e-03, -2.21195032e-03,\n",
       "         -1.24483873e-03, -3.50005427e-03,  1.85315575e-03, -2.36676512e-03,\n",
       "         -1.84856641e-03,  1.64207714e-03,  2.77179096e-04,  1.94892978e-04,\n",
       "          1.58626821e-03,  5.27858309e-03,  2.19490261e-04,  2.53899474e-03,\n",
       "         -2.30735533e-03, -9.13600825e-04,  1.14744143e-03,  4.38234859e-04,\n",
       "          1.46784387e-03,  1.10462136e-04,  5.96160211e-04, -7.12616027e-04,\n",
       "          2.26471644e-03,  1.12068162e-03,  1.58568212e-03,  4.66741030e-03,\n",
       "          1.53838559e-03, -4.14309406e-03,  2.54571758e-05,  9.86785050e-04,\n",
       "          1.90336357e-03, -2.99584627e-03, -2.57238551e-03, -3.10628027e-03,\n",
       "          3.74309194e-03,  5.79033366e-04, -3.06024149e-03, -6.81259093e-04,\n",
       "          6.76409829e-04,  1.52681367e-03,  8.48321617e-04, -1.70689858e-03,\n",
       "          3.54517327e-05,  8.65307321e-04, -8.04255409e-04,  4.52011291e-04,\n",
       "         -1.66663714e-04, -9.20264220e-04,  4.20245595e-03, -1.33796009e-03,\n",
       "         -1.02769345e-03,  1.99955175e-03,  1.67589617e-03, -2.48835902e-03,\n",
       "         -1.19783334e-03, -1.32888970e-04, -1.06546092e-03,  2.23788526e-04,\n",
       "          3.13268126e-03, -2.30978814e-03, -6.37889312e-04, -3.41634475e-03,\n",
       "         -1.73151922e-03,  6.76751517e-04, -1.06921961e-03, -1.56575269e-03,\n",
       "         -5.29577534e-04,  1.22630062e-03,  3.07288058e-03, -2.58951536e-03,\n",
       "         -1.11619496e-03, -2.31695393e-03, -4.79887010e-04,  1.31165763e-03,\n",
       "          2.54076906e-03,  6.65689610e-04, -1.40398360e-03,  1.47898415e-03,\n",
       "          2.17157874e-03,  2.65833526e-04, -7.58249432e-05, -1.25034186e-03,\n",
       "          3.82179417e-04, -4.30847411e-04, -7.76770555e-04,  1.35453661e-03,\n",
       "          1.67589549e-03,  1.12437681e-03,  1.61339016e-04, -3.59477025e-03,\n",
       "          4.95712509e-03,  2.38014634e-03, -1.50221719e-03,  2.29692902e-04,\n",
       "         -2.57855131e-03, -3.49455424e-03, -1.99788292e-03, -5.65714056e-04,\n",
       "          1.54661147e-03, -2.20397038e-03, -1.84309470e-04,  1.48328586e-03,\n",
       "          6.20545656e-04,  3.89988617e-03,  1.44318959e-03,  2.41647171e-05,\n",
       "         -1.16096855e-03, -3.99748736e-05, -1.86968893e-03,  1.75056740e-03,\n",
       "         -2.23650324e-03,  4.05936586e-03,  2.63639030e-03,  2.05213805e-03,\n",
       "          8.79745529e-05, -3.19745798e-04, -2.29836876e-03,  6.22620690e-04,\n",
       "         -2.96471773e-03,  4.58912462e-03,  3.12770770e-04,  3.55649402e-03,\n",
       "          6.77713791e-04,  3.10417427e-03, -2.36925385e-03, -7.15883040e-04,\n",
       "         -1.71045809e-03, -5.90380684e-04, -1.52586980e-03, -1.27110339e-03,\n",
       "          2.96164756e-03,  4.68582761e-04, -7.46940272e-04, -9.71294906e-04,\n",
       "         -1.25673279e-03, -2.07579088e-03, -2.17758352e-03, -5.64647897e-04,\n",
       "          2.91584960e-04, -4.02370008e-03, -4.65962308e-03, -9.15352253e-04,\n",
       "         -1.85398529e-03, -4.20799074e-03,  1.18514222e-03, -1.42119703e-03,\n",
       "         -3.14187998e-04, -9.01721370e-04, -1.07784284e-03,  1.36911377e-03,\n",
       "         -2.78691600e-03, -8.04073093e-05, -1.63460792e-04, -1.77213100e-03,\n",
       "          5.17048070e-05, -1.30670168e-03,  1.15450696e-03,  2.62707121e-03,\n",
       "          6.57438292e-04,  2.77567315e-04,  2.35681891e-05, -1.94889245e-03,\n",
       "          3.85154057e-04, -2.30318579e-03, -1.63107576e-04, -3.70382173e-04,\n",
       "         -1.80220054e-03,  2.88783991e-03,  1.10777700e-03,  1.69024484e-05,\n",
       "         -2.29278627e-03, -3.56634886e-03, -4.01887063e-03,  3.17820291e-03,\n",
       "          1.83132647e-03, -1.50165380e-03,  9.32521911e-04, -9.30600765e-04,\n",
       "         -3.93288730e-03, -1.14756994e-03, -1.11388244e-03,  9.26082148e-04,\n",
       "          1.94722190e-03, -3.35852871e-03,  7.59698934e-05,  4.49795627e-04,\n",
       "          1.15499942e-03,  2.73215523e-04, -2.30203514e-03, -1.99482052e-03,\n",
       "          3.33793591e-04, -1.51876208e-03, -4.17535925e-03, -8.53022102e-04,\n",
       "         -9.31445576e-04, -2.27143607e-04,  2.65431280e-04, -2.40917312e-03,\n",
       "         -1.39011575e-03, -2.24087518e-04, -2.61092511e-03,  2.44859233e-04,\n",
       "          2.50483475e-03,  9.09144985e-05,  8.35245591e-04, -1.15172243e-03,\n",
       "         -2.15024412e-03,  1.70014406e-03, -1.59368431e-03,  6.60644664e-04,\n",
       "          2.41486491e-04, -1.50165479e-03,  9.41081968e-04,  1.86404988e-03,\n",
       "         -7.80510842e-04, -6.56765063e-04,  3.75592412e-03,  8.63860493e-04,\n",
       "          8.28314391e-04, -3.81807770e-03,  1.38360394e-03,  9.89351353e-04,\n",
       "         -7.63560582e-05, -4.00964277e-03, -2.56520406e-04,  1.27555907e-04,\n",
       "          5.32595264e-04,  3.48244518e-03, -1.63694264e-03,  2.02111923e-03,\n",
       "         -6.86826988e-04,  1.27178499e-03, -4.50887581e-04, -3.67604850e-04,\n",
       "         -1.46360099e-03,  2.69624829e-04,  8.03605965e-04,  7.92963052e-04,\n",
       "         -3.76540661e-04, -3.39096138e-05, -1.80850430e-03, -6.40853822e-05,\n",
       "          9.73160576e-04,  3.64162454e-03,  9.25845216e-05,  3.43631164e-03,\n",
       "         -1.88758715e-05,  1.88056180e-04, -1.07159872e-03,  3.63521502e-03,\n",
       "          4.30893391e-03,  2.03888385e-03, -3.06017647e-03,  4.12534130e-03,\n",
       "         -3.63797705e-03,  1.35279594e-03, -1.26366965e-05, -2.17526724e-03,\n",
       "          9.46206046e-04,  1.40641720e-03, -5.16654901e-04,  9.43484118e-04,\n",
       "         -8.43412780e-04,  7.70245699e-04,  9.31406057e-04,  1.19285170e-03,\n",
       "         -1.86000124e-05, -4.00550708e-04, -9.44008001e-04,  2.41159242e-05,\n",
       "         -1.55043482e-03,  4.30922057e-05, -1.85983303e-03, -3.73122767e-03,\n",
       "          1.58209694e-03,  2.65191315e-05, -3.97170105e-03, -9.56365621e-04,\n",
       "         -2.75165298e-03,  1.94538358e-03,  3.28370098e-03,  1.67647929e-03,\n",
       "         -2.69647815e-03,  2.66997128e-03, -5.80500370e-05,  2.03959401e-03,\n",
       "         -2.75455046e-03, -1.51807992e-03,  2.94039304e-03, -5.87717467e-05,\n",
       "         -2.10567000e-03,  9.08511778e-04, -2.82167618e-03, -1.73390706e-03,\n",
       "          1.05530034e-04, -3.29405635e-04,  8.47281069e-04,  2.14116660e-04,\n",
       "          4.14191705e-04, -1.36332053e-03, -1.63148115e-03,  5.61201271e-04,\n",
       "          5.30262133e-03, -1.43479329e-03, -2.23265694e-03,  1.49024191e-03,\n",
       "         -1.71582066e-03, -1.20570201e-03, -6.52758392e-04, -2.27551221e-03,\n",
       "         -2.08848083e-03, -2.52582495e-04,  1.21727772e-03, -6.65330018e-07,\n",
       "         -3.78542816e-04,  1.30784746e-03,  1.74357859e-03,  1.14468291e-04,\n",
       "          2.16595782e-03, -8.01835193e-04,  9.35541782e-04,  2.91712912e-04,\n",
       "         -1.34527897e-03,  1.39526919e-03,  9.28081339e-04, -1.00175086e-04,\n",
       "         -3.23421430e-04,  3.64871301e-03, -3.77312356e-03, -2.22666082e-03,\n",
       "          1.65024740e-03,  3.35708721e-03, -1.46420512e-03, -5.68015734e-04,\n",
       "         -2.83778111e-03,  3.13389327e-03, -9.92265815e-04,  9.27782437e-04,\n",
       "          1.69516136e-03, -3.94989742e-04,  1.16821070e-05,  6.04608955e-03,\n",
       "          3.78521106e-03,  1.03267126e-03, -3.32521369e-04, -1.34474240e-03,\n",
       "         -2.69007727e-03,  3.07692534e-04, -7.82349344e-04,  1.64619507e-03,\n",
       "         -8.23954195e-04, -7.15302260e-04,  2.70771637e-03,  1.94326914e-03,\n",
       "          8.71711102e-04, -9.22744581e-04, -2.87278428e-03,  4.65175595e-03,\n",
       "         -3.15633510e-03, -2.81214596e-03,  5.36557367e-04,  2.56249948e-03,\n",
       "          1.80311396e-03,  8.51925571e-04,  1.90766055e-03,  1.97286291e-03,\n",
       "          9.65919032e-04, -4.72199153e-04,  2.52796917e-03,  1.11277807e-03,\n",
       "          1.31600085e-03, -9.72116974e-04,  2.88786661e-04, -2.09549079e-03,\n",
       "          5.70362176e-04, -1.53485858e-03, -9.98851464e-04,  3.21935510e-03,\n",
       "          1.90624743e-03, -9.13924236e-04, -4.15872096e-03, -4.10003689e-03,\n",
       "         -2.54408190e-03,  4.05150221e-03, -1.35514998e-03, -1.69529358e-03,\n",
       "          2.73250032e-04, -8.54136211e-05, -1.12241502e-03,  2.46118182e-03,\n",
       "          2.31725733e-03, -5.07279520e-03,  1.17397097e-03,  3.20750156e-03,\n",
       "          1.62118437e-05,  2.77484286e-03,  3.83660246e-03,  1.38950848e-05,\n",
       "          1.17707248e-04, -2.14508108e-03, -5.31714762e-04,  2.85795698e-03,\n",
       "         -1.92912780e-03,  2.54358805e-04,  4.04312209e-04,  1.76740679e-03,\n",
       "          4.62617348e-03,  8.82854331e-04, -4.46089677e-03, -3.40157279e-03,\n",
       "          1.51913336e-04, -2.74826887e-03,  1.60114262e-03, -3.79857600e-03,\n",
       "         -3.92199491e-03, -5.41503375e-03,  4.12089235e-04,  2.11939688e-03,\n",
       "          2.14518394e-03,  3.26551813e-03, -2.45025382e-03,  3.45544004e-03])},\n",
       " {'b': array([ 0.01059746,  0.01189852, -0.0101022 ,  0.01236293, -0.0370566 ,\n",
       "          0.00220618, -0.00471306,  0.00952592, -0.0027816 ,  0.00806246]),\n",
       "  'w': array([[ 0.07763946, -0.07490415,  0.02672727, ...,  0.0228989 ,\n",
       "           0.04487334, -0.05341886],\n",
       "         [-0.02810506, -0.0454346 ,  0.00293397, ...,  0.02890079,\n",
       "          -0.03008617, -0.06624106],\n",
       "         [-0.00448756,  0.00352539, -0.05803798, ..., -0.04088229,\n",
       "           0.01243871, -0.06583572],\n",
       "         ...,\n",
       "         [ 0.07926408, -0.04220151, -0.0144447 , ...,  0.03591169,\n",
       "           0.06232407,  0.00545332],\n",
       "         [-0.01329172,  0.05197449,  0.00073675, ...,  0.06219697,\n",
       "          -0.08225845,  0.04276591],\n",
       "         [ 0.04130035, -0.07863287, -0.03081868, ...,  0.01858193,\n",
       "          -0.04061014,  0.05589274]])}]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "train_batch(0, parameters)\n",
    "\n",
    "combine_parameters(parameters, train_batch(0, parameters), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 15.1 %\n"
     ]
    }
   ],
   "source": [
    "### 训练过程\n",
    "\n",
    "parameters = init_parameters()\n",
    "\n",
    "for i in range(trian_num // batch_size):\n",
    "    if i % 100 == 99:\n",
    "        print(\"runing batch {}/{}\".format(i+1, trian_num // batch_size))\n",
    "    grad_tmp = train_batch(i, parameters)\n",
    "    parameters = combine_parameters(parameters, grad_tmp, 0.01)\n",
    "valid_accuracy(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寻找较优learn_rate\n",
    "\n",
    "def train_for_better():\n",
    "    param = 0.1\n",
    "    for j in range(300):\n",
    "        parameters = init_parameters()\n",
    "        for i in range(trian_num // batch_size):\n",
    "            if i % 100 == 99: \n",
    "                print(\"runing batch {}/{}\".format(i+1, trian_num // batch_size))\n",
    "            grad_tmp = train_batch(i, parameters) \n",
    "            temp = param + 0.1\n",
    "            parameters = combine_parameters(parameters, grad_tmp, temp)\n",
    "        valid_accuracy(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.17 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.53 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.89 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.07 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.79 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.44 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.34 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.919999999999995 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.769999999999996 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.71 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.26 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.62 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.76 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.94 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.88 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.1 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.1 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.480000000000004 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.34 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.82 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.13 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.1 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.69 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.72 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.269999999999996 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.36 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.32 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.580000000000005 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.97 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.76 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.22 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.66 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.04 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.99 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.32 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.37 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.89 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.34 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.3 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.33 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.769999999999996 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.72 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.67 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.16 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.169999999999995 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.85 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.86 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.84 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.33 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.28 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.41 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.01 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.57 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.18 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.830000000000005 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.519999999999996 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.33 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.97 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.059999999999995 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.379999999999995 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.47 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.5 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.72 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.519999999999996 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.06 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.13 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.25 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.81 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.68 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.49 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.33 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.56 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.93 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.28 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.32 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.85 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.99 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.85 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.230000000000004 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.71 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.93 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.83 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.62 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 45.2 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.22 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.03 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.54 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.18 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.0 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.28 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.0 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.29 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.01 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.28 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.169999999999995 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.4 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.51 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.25 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.89 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.18 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.49 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.91 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.44 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.14 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.25 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.79 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.71 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.38 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.4 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.84 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.36 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.29 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.79 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.72 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.61 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.0 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.1 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.25 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.49 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.97 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 42.94 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.32 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.97 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.269999999999996 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.169999999999995 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.14 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.41 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.37 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.12 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.05 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.25 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.28 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.19 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.66 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.980000000000004 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.57 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.53 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.43 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.669999999999995 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.02 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.309999999999995 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.61 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.01 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.419999999999995 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.53 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.84 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 44.01 %\n",
      "runing batch 100/500\n",
      "runing batch 200/500\n",
      "runing batch 300/500\n",
      "runing batch 400/500\n",
      "runing batch 500/500\n",
      "validation accuracy: 43.64 %\n"
     ]
    }
   ],
   "source": [
    "train_for_better()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}